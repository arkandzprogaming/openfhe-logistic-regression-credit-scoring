{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54bc0082",
   "metadata": {},
   "source": [
    "# Goal:\n",
    "\n",
    "- provide a plaintext interface to analyze step-by-step what is happening in the encrypted code\n",
    "\n",
    "- Used as a Python sanity check because I'm not that familiar with R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91303fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'raise', 'over': 'raise', 'under': 'raise', 'invalid': 'raise'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "from numba import njit\n",
    "\n",
    "np.seterr(all='raise')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e8a827",
   "metadata": {},
   "source": [
    "# Load and Process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31e34725-f22a-4ce4-bc21-3967727b0d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "\n",
    "\n",
    "def load_data(num_samples, compare_to_r_ref):\n",
    "    x_file = \"../train_data/X_norm_1024.csv\"\n",
    "    y_file = \"../train_data/y_1024.csv\"\n",
    "    train_x = pd.read_csv(x_file)\n",
    "    train_x = train_x.to_numpy()[:num_samples]\n",
    "    train_y = pd.read_csv(y_file)\n",
    "\n",
    "    class_counts = train_y.iloc[:, 0].value_counts()\n",
    "    print(class_counts)\n",
    "\n",
    "    # If it's a binary classification, calculate the ratio\n",
    "    positive_class = class_counts[1.0]  \n",
    "    negative_class = class_counts[0.0]  \n",
    "    ratio = positive_class / negative_class\n",
    "    \n",
    "    print(f\"Positive class: {positive_class}, Negative class: {negative_class}, Ratio: {ratio:.2f}\")\n",
    "      \n",
    "    train_y = train_y.to_numpy()[:num_samples]\n",
    "    print(f\"{bcolors.OKGREEN}Using subsampled data to compare Python-C++{bcolors.ENDC}\")\n",
    "    print(f\"{bcolors.OKGREEN}Reading in {x_file}, {y_file} {bcolors.ENDC}\")\n",
    "\n",
    "    print(f\"Train X shape is: {train_x.shape}\")\n",
    "    print(f\"Train y shape is: {train_y.shape}\")\n",
    "    return train_x, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ce32218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMORT\n",
      "1    512\n",
      "0    512\n",
      "Name: count, dtype: int64\n",
      "Positive class: 512, Negative class: 512, Ratio: 1.00\n",
      "\u001b[92mUsing subsampled data to compare Python-C++\u001b[0m\n",
      "\u001b[92mReading in ../train_data/X_norm_1024.csv, ../train_data/y_1024.csv \u001b[0m\n",
      "Train X shape is: (1023, 10)\n",
      "Train y shape is: (1023, 1)\n"
     ]
    }
   ],
   "source": [
    "NUM_SAMPLES = -1\n",
    "COMPARE_TO_R_REF = False\n",
    "lr = 0.1\n",
    "mu = 0.1\n",
    "train_x, train_y = load_data(\n",
    "    num_samples=NUM_SAMPLES,\n",
    "    compare_to_r_ref=COMPARE_TO_R_REF\n",
    ")\n",
    "\n",
    "# Same shape as Marcelo's reference code\n",
    "betas = np.zeros((train_x.shape[1], ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ac264a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def fwd(train_x, betas, dbg=False):\n",
    "    preds = train_x @ betas   # A vector of linear_predictions/logits z = train_x @ weights\n",
    "    if dbg:\n",
    "        print(f\"Logits: {preds}\")\n",
    "    return np.expand_dims(sigmoid(preds), -1)   # Shape: (m, 1)\n",
    "\n",
    "def calculate_gradient(train_x, train_y, betas, fwd, dbg):\n",
    "    preds = fwd(train_x, betas, dbg)   # A vector of logistic predictions y_hat = sigmoid(z)\n",
    "    gradient = -train_x.T @ (train_y - preds) / len(train_y)\n",
    "    return gradient   # Shape: (10, 1) == Rows correspond to num_features\n",
    "    ## This function is used to update the values of betas: w_new = w_old + lr * gradient\n",
    "\n",
    "def cost(x, y, theta):\n",
    "    m = x.shape[0]\n",
    "    h = sigmoid(np.matmul(x, theta))   # h: hypothesis, basically preds/y_hat\n",
    "    t1 = np.matmul(-y.T, np.log(h))\n",
    "    t2_a = (1 - y.T)\n",
    "    t2_b = np.log(np.clip(1 - h, 0.000000000000001, np.max(1 - h)))  # Used to get numerical issues\n",
    "    ## np.clip() function prevents computing the log of 0, by taking the minimum of 1e-15.\n",
    "    t2 = np.matmul(t2_a, t2_b)\n",
    "\n",
    "    return ((t1 - t2) / m)[0]   # Shape: (1,) == scalar value\n",
    "\n",
    "def nesterov(betas, epochs, lr, mu, train_x, train_y):\n",
    "    import copy\n",
    "\n",
    "    phi = copy.deepcopy(betas)\n",
    "    theta = copy.deepcopy(betas)\n",
    "\n",
    "    nesterov_loss = [0 for _ in range(epochs)]\n",
    "    for i in tqdm.trange(epochs):\n",
    "    # for i in range(epochs):\n",
    "        gradient = calculate_gradient(train_x, train_y, theta, fwd, dbg=False)\n",
    "\n",
    "        ## Assign updated weights into phi_prime\n",
    "        phi_prime = theta - lr * np.squeeze(gradient)   # np.squeeze() removes single dimensions --> shape (10,)\n",
    "        \n",
    "        ## Nesterov acceleration process\n",
    "        if i == 0:\n",
    "            theta = phi_prime\n",
    "        else:\n",
    "            ## If current updated weight (phi_prime) < previous weight (phi), \n",
    "            ## The updated weight theta will be even smaller.\n",
    "            theta = phi_prime + mu * (phi_prime - phi)\n",
    "        phi = phi_prime   # phi is then the weight of the previous epoch/update\n",
    "        loss = cost(train_x, train_y, theta)\n",
    "        nesterov_loss[i] = loss\n",
    "\n",
    "        # print(f\"New loss: {cost(train_x, train_y, v)[0]}\")\n",
    "    return nesterov_loss, theta, phi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c74a3d18-2a23-46e6-897a-2dc859beb572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 2444.08it/s]\n"
     ]
    }
   ],
   "source": [
    "losses, theta, phi = nesterov(betas, 200, lr, mu, train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "745a6998",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(0.682438146439526),\n",
       " np.float64(0.6714453914072441),\n",
       " np.float64(0.6611013485147301),\n",
       " np.float64(0.6514162146586544),\n",
       " np.float64(0.6423259325233592),\n",
       " np.float64(0.6337707531714274),\n",
       " np.float64(0.6257002096176623),\n",
       " np.float64(0.6180716210017204),\n",
       " np.float64(0.6108484558248058),\n",
       " np.float64(0.6039990483572646),\n",
       " np.float64(0.5974956250378402),\n",
       " np.float64(0.5913135670891185),\n",
       " np.float64(0.5854308495746274),\n",
       " np.float64(0.5798276116655406),\n",
       " np.float64(0.5744858243802814),\n",
       " np.float64(0.5693890307953972),\n",
       " np.float64(0.5645221402767862),\n",
       " np.float64(0.5598712631529988),\n",
       " np.float64(0.5554235758543887),\n",
       " np.float64(0.5511672091916684),\n",
       " np.float64(0.5470911543892155),\n",
       " np.float64(0.5431851829072069),\n",
       " np.float64(0.5394397771210908),\n",
       " np.float64(0.5358460696801214),\n",
       " np.float64(0.532395789914908),\n",
       " np.float64(0.5290812160631745),\n",
       " np.float64(0.525895132374097),\n",
       " np.float64(0.5228307903645797),\n",
       " np.float64(0.5198818736572505),\n",
       " np.float64(0.5170424659455463),\n",
       " np.float64(0.5143070217173022),\n",
       " np.float64(0.5116703394329533),\n",
       " np.float64(0.5091275369036673),\n",
       " np.float64(0.5066740286526822),\n",
       " np.float64(0.5043055050728511),\n",
       " np.float64(0.5020179132170797),\n",
       " np.float64(0.4998074390775531),\n",
       " np.float64(0.49767049122548795),\n",
       " np.float64(0.4956036856964576),\n",
       " np.float64(0.4936038320176769),\n",
       " np.float64(0.49166792028344497),\n",
       " np.float64(0.4897931091935313),\n",
       " np.float64(0.48797671497688677),\n",
       " np.float64(0.48621620112983177),\n",
       " np.float64(0.48450916890395523),\n",
       " np.float64(0.48285334848444694),\n",
       " np.float64(0.4812465908045594),\n",
       " np.float64(0.4796868599464127),\n",
       " np.float64(0.47817222608248233),\n",
       " np.float64(0.47670085891585734),\n",
       " np.float64(0.4752710215808071),\n",
       " np.float64(0.4738810649683188),\n",
       " np.float64(0.4725294224441625),\n",
       " np.float64(0.47121460492965656),\n",
       " np.float64(0.46993519631772757),\n",
       " np.float64(0.46868984919906936),\n",
       " np.float64(0.4674772808752217),\n",
       " np.float64(0.4662962696372538),\n",
       " np.float64(0.4651456512904337),\n",
       " np.float64(0.4640243159068247),\n",
       " np.float64(0.46293120478917876),\n",
       " np.float64(0.46186530763080974),\n",
       " np.float64(0.4608256598573272),\n",
       " np.float64(0.45981134013721975),\n",
       " np.float64(0.45882146804928364),\n",
       " np.float64(0.457855201895823),\n",
       " np.float64(0.45691173665140083),\n",
       " np.float64(0.45599030203770025),\n",
       " np.float64(0.4550901607157768),\n",
       " np.float64(0.4542106065876423),\n",
       " np.float64(0.45335096319972273),\n",
       " np.float64(0.4525105822413017),\n",
       " np.float64(0.45168884213155963),\n",
       " np.float64(0.45088514668930424),\n",
       " np.float64(0.4500989238799125),\n",
       " np.float64(0.44932962463441173),\n",
       " np.float64(0.44857672173598895),\n",
       " np.float64(0.4478397087695645),\n",
       " np.float64(0.44711809913037265),\n",
       " np.float64(0.4464114250877841),\n",
       " np.float64(0.4457192369008733),\n",
       " np.float64(0.4450411019824721),\n",
       " np.float64(0.4443766041086903),\n",
       " np.float64(0.44372534267108166),\n",
       " np.float64(0.4430869319688345),\n",
       " np.float64(0.44246100053854703),\n",
       " np.float64(0.44184719051930554),\n",
       " np.float64(0.4412451570509492),\n",
       " np.float64(0.44065456770353484),\n",
       " np.float64(0.4400751019361563),\n",
       " np.float64(0.43950645058339377),\n",
       " np.float64(0.43894831536777634),\n",
       " np.float64(0.4384004084367534),\n",
       " np.float64(0.4378624519227642),\n",
       " np.float64(0.4373341775250872),\n",
       " np.float64(0.4368153261122332),\n",
       " np.float64(0.4363056473437283),\n",
       " np.float64(0.435804899310201),\n",
       " np.float64(0.4353128481907611),\n",
       " np.float64(0.43482926792671506),\n",
       " np.float64(0.4343539399107274),\n",
       " np.float64(0.4338866526905878),\n",
       " np.float64(0.4334272016867975),\n",
       " np.float64(0.43297538892323534),\n",
       " np.float64(0.43253102277020833),\n",
       " np.float64(0.432093917699233),\n",
       " np.float64(0.43166389404893324),\n",
       " np.float64(0.4312407778014747),\n",
       " np.float64(0.43082440036899533),\n",
       " np.float64(0.43041459838951257),\n",
       " np.float64(0.4300112135318291),\n",
       " np.float64(0.42961409230898134),\n",
       " np.float64(0.42922308589979796),\n",
       " np.float64(0.42883804997816644),\n",
       " np.float64(0.4284588445496234),\n",
       " np.float64(0.4280853337949092),\n",
       " np.float64(0.42771738592014497),\n",
       " np.float64(0.4273548730133096),\n",
       " np.float64(0.42699767090671387),\n",
       " np.float64(0.4266456590451824),\n",
       " np.float64(0.42629872035967165),\n",
       " np.float64(0.4259567411460676),\n",
       " np.float64(0.4256196109489159),\n",
       " np.float64(0.4252872224498598),\n",
       " np.float64(0.4249594713605589),\n",
       " np.float64(0.4246362563198902),\n",
       " np.float64(0.424317478795229),\n",
       " np.float64(0.4240030429876248),\n",
       " np.float64(0.4236928557406986),\n",
       " np.float64(0.42338682645309006),\n",
       " np.float64(0.4230848669942973),\n",
       " np.float64(0.4227868916237597),\n",
       " np.float64(0.42249281691303686),\n",
       " np.float64(0.4222025616709514),\n",
       " np.float64(0.4219160468715637),\n",
       " np.float64(0.421633195584856),\n",
       " np.float64(0.42135393291000894),\n",
       " np.float64(0.4210781859111601),\n",
       " np.float64(0.4208058835555367),\n",
       " np.float64(0.4205369566538642),\n",
       " np.float64(0.4202713378029527),\n",
       " np.float64(0.42000896133037013),\n",
       " np.float64(0.4197497632411169),\n",
       " np.float64(0.4194936811662164),\n",
       " np.float64(0.4192406543131446),\n",
       " np.float64(0.4189906234180225),\n",
       " np.float64(0.4187435306994985),\n",
       " np.float64(0.4184993198142539),\n",
       " np.float64(0.4182579358140661),\n",
       " np.float64(0.4180193251043635),\n",
       " np.float64(0.41778343540421903),\n",
       " np.float64(0.4175502157077186),\n",
       " np.float64(0.4173196162466545),\n",
       " np.float64(0.41709158845449035),\n",
       " np.float64(0.4168660849315475),\n",
       " np.float64(0.41664305941136615),\n",
       " np.float64(0.41642246672819627),\n",
       " np.float64(0.41620426278557304),\n",
       " np.float64(0.4159884045259373),\n",
       " np.float64(0.41577484990126085),\n",
       " np.float64(0.41556355784463683),\n",
       " np.float64(0.41535448824280224),\n",
       " np.float64(0.4151476019095538),\n",
       " np.float64(0.41494286056002655),\n",
       " np.float64(0.4147402267858033),\n",
       " np.float64(0.4145396640308214),\n",
       " np.float64(0.4143411365680507),\n",
       " np.float64(0.4141446094769139),\n",
       " np.float64(0.41395004862142043),\n",
       " np.float64(0.4137574206289899),\n",
       " np.float64(0.4135666928699402),\n",
       " np.float64(0.4133778334376149),\n",
       " np.float64(0.41319081112913),\n",
       " np.float64(0.41300559542671345),\n",
       " np.float64(0.41282215647962256),\n",
       " np.float64(0.41264046508661334),\n",
       " np.float64(0.4124604926789451),\n",
       " np.float64(0.4122822113039036),\n",
       " np.float64(0.4121055936088193),\n",
       " np.float64(0.4119306128255692),\n",
       " np.float64(0.41175724275554254),\n",
       " np.float64(0.4115854577550559),\n",
       " np.float64(0.41141523272119995),\n",
       " np.float64(0.4112465430781079),\n",
       " np.float64(0.41107936476362616),\n",
       " np.float64(0.4109136742163777),\n",
       " np.float64(0.41074944836320193),\n",
       " np.float64(0.4105866646069614),\n",
       " np.float64(0.41042530081470147),\n",
       " np.float64(0.4102653353061502),\n",
       " np.float64(0.41010674684255083),\n",
       " np.float64(0.40994951461581247),\n",
       " np.float64(0.4097936182379704),\n",
       " np.float64(0.4096390377309449),\n",
       " np.float64(0.40948575351659033),\n",
       " np.float64(0.4093337464070244),\n",
       " np.float64(0.4091829975952278),\n",
       " np.float64(0.4090334886459068),\n",
       " np.float64(0.4088852014866103),\n",
       " np.float64(0.4087381183990923)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses\n",
    "\n",
    "## zero initialized betas    : cost = 0.4087381183990924\n",
    "## uniform initialized betas : cost = 0.40867096414830645\n",
    "## randn initialized betas   : cost = 0.40850863604124726"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a1f5f95-8319-4c52-96f7-9accd6387d14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.24842111,  0.19234508, -1.19103479,  0.43030586, -0.64449037,\n",
       "       -0.04041207, -0.0847023 , -0.36362034,  0.11810004,  0.44677751])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f32b31a-e349-435e-a5b9-e478f7ce2c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = fwd(train_x, theta, dbg=False)\n",
    "\n",
    "## Decision (Threshold = 0.5)\n",
    "train_y_hat = (pred >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c2c4881-5393-4543-af7a-35d8911bf7f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1023, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19a522e7-33c1-446e-9623-967aba4218e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1023, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1eb4254f-d93b-43cd-bfe5-4934efbfbf17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[463,  48],\n",
       "       [136, 376]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(train_y, train_y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86e88972-a16c-4d19-a14a-935961fcd0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8901472296966731)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(train_y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "720e704c-80aa-4b39-8fde-4fae7367bb6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.820136852394917"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(train_y, train_y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac20ef3c-c366-4bd1-a272-cd3b762ee71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8034188034188035"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(train_y, train_y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec9e858-97f0-4061-919c-acaf63e86d5a",
   "metadata": {},
   "source": [
    "On test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c873d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(train_y, pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
